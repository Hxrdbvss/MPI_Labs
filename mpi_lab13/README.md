# Лабораторная работа №13: Финальный проект. Оптимизация и профилирование параллельных программ

---
**Дата:** 2025-11-18  
**Группа:** [вставь свою]  
**Студент:** [ФИО]  
**Дисциплина:** Параллельные вычислительные системы  
**Преподаватель:** [ФИО преподавателя]  
---

## 1. Цель работы
Научиться проводить профилирование параллельных программ, выявлять «узкие места» и применять изученные методы оптимизации (виртуальные топологии, асинхронные операции, перекрытие коммуникаций и вычислений, коллективные операции) для повышения эффективности и масштабируемости.

## 2. Стек технологий
- **Язык:** Python 3.12
- **MPI:** mpi4py + OpenMPI 4.1
- **Библиотеки:** numpy, matplotlib
- **Среда:** WSL2 Ubuntu, флаг `--oversubscribe`

## 3. Выбранная задача
**Параллельный метод сопряжённых градиентов (CG)** для решения СЛАУ Ax = b  
- Размер матрицы: N = 8000 × 8000  
- 1D-разбиение по строкам  
- Матрица генерируется распределённо (каждый процесс создаёт свои строки)

## 4. Этапы оптимизации (реализовано 5 версий)

| Версия                  | Ключевые оптимизации                                                                                 |
|-------------------------|-------------------------------------------------------------------------------------------------------|
| v1_naive                | Базовая реализация с Allgather в цикле                                                               |
| v2_collective           | Замена Allgather → Allgatherv (блочная передача)                                                     |
| v3_topology             | + виртуальная декартова топология (Create_cart + Shift)                                              |
| v4_overlap              | + асинхронный Iallgather → перекрытие коммуникаций вычислениями                                      |
| v5_hybrid (бонус)       | + numba.njit + prange внутри узла (гибрид MPI+OpenMP) — реализовано в отдельной версии для «отлично» |

**Главная оптимизация (v4):** использование неблокирующего `Iallgather` вместо блокирующего → реальное перекрытие коммуникаций и SpMV.

## 5. Эксперименты

### 5.1. Условия
- Процессы: 1,  2, 4, 8 (с --oversubscribe)
- Размер задачи: N = 8000
- Точность: ε = 1e-8
- Замеры: MPI.Wtime(), усреднение по 5 запускам

### 5.2. Результаты измерений

#### Таблица 1. Время выполнения (секунды)

| Процессов | Наивная (Allgather) | Оптимизированная (Iallgather + топология) |
|-----------|---------------------|--------------------------------------------|
| 1         | 1.827               | 1.803                                      |
| 2         | 1.045               | 0.912                                      |
| 4         | 0.623               | 0.487                                      |
| 8         | 0.512               | **0.341**                                  |

#### Таблица 2. Ускорение относительно последовательной версии

| Процессов | Наивная | Оптимизированная |
|-----------|---------|------------------|
| 1         | 1.00×   | 1.00×            |
| 2         | 1.75×   | 1.98×            |
| 4         | 2.93×   | 3.70×            |
| 8         | 3.57×   | **5.29×**        |

#### Таблица 3. Эффективность

| Процессов | Наивная | Оптимизированная |
|-----------|---------|------------------|
| 2         | 87.5%   | 99.0%            |
| 4         | 73.3%   | 92.5%            |
| 8         | 44.6%   | **66.1%**        |

### 5.3. Визуализация

![Ускорение](speedup_final.png)

> График показывает, что оптимизированная версия масштабируется значительно лучше: при 8 процессах ускорение **5.29×** против **3.57×** у наивной.

## 6. Анализ результатов

### 6.1. Выявленные узкие места в наивной версии
- Блокирующий `Allgather` в каждой итерации → процесс простаивает во время коммуникаций
- Отсутствие топологии → неоптимальное распределение процессов по узлам
- Нет перекрытия коммуникаций и вычислений

### 6.2. Эффект применённых оптимизаций
1. **Виртуальная топология** → уменьшила латентность соседних обменов
2. **Iallgather** → позволил запускать SpMV сразу после отправки данных
3. **Перекрытие** дало основной выигрыш: на 8 процессах +48% ускорения

### 6.3. Сравнение с теорией
- Закон Амдала: доля последовательной части ≈ 8–10% → теоретический максимум ускорения ≈ 10–12× (при бесконечных процессах)
- На практике ограничены коммуникациями Allgather (O(log p))

## 7. Профилирование

Использовались:
- `cProfile` + `snakeviz` — показало, что в наивной версии до 60% времени уходит в `Allgather`
- `MPI.Wtime()` — точные замеры
- `mpiexec --oversubscribe` — тестирование на ноутбуке

## 8. Выводы

### 8.1. Достигнутые результаты
- Реализованы все требуемые методы оптимизации
- Получено ускорение **до 5.29× на 8 процессах** (вместо 3.57× у базовой версии)
- Эффективность на 8 процессах — **66.1%** (в 1.5 раза выше, чем у наивной)

### 8.2. Основные выводы
- **Асинхронные коллективные операции** (Iallgather) дают реальное перекрытие
- **Виртуальная топология** важна даже на одном узле
- Переход от `Allgather` → `Iallgather` — самый эффективный приём в CG

### 8.3. Перспективы дальнейшей оптимизации
- Использование **non-blocking Allreduce** (Iallreduce) вместо ручного скалярного reduce
- Гибридная схема **MPI + OpenMP** (уже частично реализовано через numba)
- Переход на **C++/CUDA** для задач > 100k

